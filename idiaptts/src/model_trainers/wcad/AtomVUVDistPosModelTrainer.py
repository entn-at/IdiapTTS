#!/usr/bin/env python3
# -*- coding: utf-8 -*-
#
# Copyright (c) 2019 Idiap Research Institute, http://www.idiap.ch/
# Written by Bastian Schnell <bastian.schnell@idiap.ch>
#

"""Module description:
   Train a model to predict atoms and generate LF0 from them.
   Format is T x (|thetas| + 1) with one amplitude per theta and a position flag at last position.
   Each amplitude in the target labels is surrounded by a distribution.
   Combine LF0 data with external MGC and BAP data to synthesize audio.
"""

# System imports.
import logging
import math
import os
import sys
import numpy as np

# Third-party imports.

# Local source tree imports.
from idiaptts.src.model_trainers.ModelTrainer import ModelTrainer
from idiaptts.src.model_trainers.wcad.AtomModelTrainer import AtomModelTrainer
from idiaptts.src.data_preparation.questions.QuestionLabelGen import QuestionLabelGen
from idiaptts.src.data_preparation.wcad.AtomVUVDistPosLabelGen import AtomVUVDistPosLabelGen
from idiaptts.src.data_preparation.PyTorchLabelGensDataset import PyTorchLabelGensDataset
from idiaptts.src.data_preparation.world.LF0LabelGen import LF0LabelGen
from idiaptts.src.data_preparation.wcad.AtomLabelGen import AtomLabelGen
from idiaptts.src.data_preparation.world.WorldFeatLabelGen import WorldFeatLabelGen
from idiaptts.src.neural_networks.pytorch.loss.WeightedNonzeroWMSEAtomLoss import WeightedNonzeroWMSEAtomLoss
from idiaptts.src.DataPlotter import DataPlotter
from idiaptts.misc.utils import interpolate_lin


class AtomVUVDistPosModelTrainer(AtomModelTrainer):
    """
    Subclass of AtomModelTrainer, which uses one amplitude per theta plus position flag,
    format is T x (|thetas| + 1). Each amplitude in the target labels is surrounded by a distribution.
    Positions of atoms are identified by finding the peaks of the position flag prediction. For positive peaks
    the theta with the highest amplitude is used, for negative peaks the theta with the lowest amplitude.
    Acoustic data is generated from these atoms. MGC and BAP is either generated by a pre-trained acoustic model
    or loaded from the original extracted files. Question labels are used as input.
    """
    logger = logging.getLogger(__name__)

    def __init__(self, wcad_root, dir_atom_labels, dir_lf0_labels, dir_question_labels, id_list,
                 thetas, k,
                 num_questions,
                 dist_window_size=51, hparams=None):
        """Default constructor.

        :param wcad_root:               Path to main directory of wcad.
        :param dir_atom_labels:         Path to directory that contains the .wav files.
        :param dir_lf0_labels:          Path to directory that contains the .lf0 files.
        :param dir_question_labels:     Path to directory that contains the .lab files.
        :param id_list:                 List containing all ids. Subset is taken as test set.
        :param thetas:                  List of theta values of atoms.
        :param k:                       K-value of atoms.
        :param num_questions:           Expected number of questions in question labels.
        :param dist_window_size:        Width of the distribution surrounding each atom spike
                                        The window is only used for amps. Thetas are surrounded by a window of 5.
        :param hparams:                 Hyper-parameter container.
        """
        if hparams is None:
            hparams = self.create_hparams()
            hparams.out_dir = os.path.curdir

        # Write missing default parameters.
        if hparams.variable_sequence_length_train is None:
            hparams.variable_sequence_length_train = hparams.batch_size_train > 1
        if hparams.variable_sequence_length_test is None:
            hparams.variable_sequence_length_test = hparams.batch_size_test > 1
        if hparams.synth_dir is None:
            hparams.synth_dir = os.path.join(hparams.out_dir, "synth")

        # If the weight for unvoiced frames is not given, compute it to get equal weights.
        if not hasattr(hparams, "weight_zero") or hparams.weight_zero is None:
            non_zero_occurrence = min(0.99, 0.015 / len(thetas))
            zero_occurrence = 1 - non_zero_occurrence
            hparams.weight_non_zero = 1 / non_zero_occurrence
            hparams.weight_zero = 1 / zero_occurrence
        if not hasattr(hparams, "weight_vuv") or hparams.weight_vuv is None:
            hparams.weight_vuv = 0.5
        if not hasattr(hparams, "atom_loss_theta") or hparams.atom_loss_theta is None:
            hparams.atom_loss_theta = 0.01

        # Explicitly call only the constructor of the baseclass of AtomModelTrainer.
        super(AtomModelTrainer, self).__init__(id_list, hparams)

        if hparams.dist_window_size % 2 == 0:
            hparams.dist_window_size += 1
            self.logger.warning("hparams.dist_window_size should be odd, changed it to " + str(hparams.dist_window_size))

        self.InputGen = QuestionLabelGen(dir_question_labels, num_questions)
        self.InputGen.get_normalisation_params(dir_question_labels)

        # Overwrite OutputGen by the one with beta distribution.
        self.OutputGen = AtomVUVDistPosLabelGen(wcad_root, dir_atom_labels, dir_lf0_labels, thetas, k, hparams.frame_size_ms, window_size=dist_window_size)
        self.OutputGen.get_normalisation_params(dir_atom_labels)

        self.dataset_train = PyTorchLabelGensDataset(self.id_list_train, self.InputGen, self.OutputGen, hparams, length_check=True)
        self.dataset_val = PyTorchLabelGensDataset(self.id_list_val, self.InputGen, self.OutputGen, hparams, length_check=True)

        if self.loss_function is None:
            self.loss_function = WeightedNonzeroWMSEAtomLoss(hparams.use_gpu, hparams.atom_loss_theta, hparams.weight_vuv, hparams.weight_zero, hparams.weight_non_zero, reduce=False)

        if hparams.scheduler_type == "default":
            hparams.scheduler_type = "None"

    @staticmethod
    def create_hparams(hparams_string=None, verbose=False):
        hparams = AtomModelTrainer.create_hparams(hparams_string, verbose=False)
        hparams.dist_window_size = 51

        if verbose:
            logging.info('Final parsed hparams: %s', hparams.values())

        return hparams

    def gen_figure_from_output(self, id_name, label, hidden, hparams):

        # Retrieve data from label.
        output_amps = label[:, 1:-1]
        output_pos = label[:, -1]
        labels_post = self.OutputGen.postprocess_sample(label)
        output_vuv = labels_post[:, 0, 1].astype(bool)
        output_atoms = self.OutputGen.labels_to_atoms(labels_post, k=hparams.k, amp_threshold=hparams.min_atom_amp)
        output_lf0 = self.OutputGen.atoms_to_lf0(output_atoms, len(label))

        # Load original lf0 and vuv.
        org_labels = LF0LabelGen.load_sample(id_name, os.path.join(hparams.out_dir, self.dir_extracted_acoustic_features))
        original_lf0, _ = LF0LabelGen.convert_to_world_features(org_labels)
        original_lf0, _ = interpolate_lin(original_lf0)

        phrase_curve = np.fromfile(os.path.join(self.OutputGen.dir_labels, id_name + self.OutputGen.ext_phrase),
                                   dtype=np.float32).reshape(-1, 1)
        original_lf0[:len(phrase_curve)] -= phrase_curve[:len(original_lf0)]
        original_lf0 = original_lf0[:len(output_lf0)]

        org_labels = self.OutputGen.load_sample(id_name,
                                                self.OutputGen.dir_labels,
                                                len(hparams.thetas),
                                                self.OutputGen.dir_world_labels)
        org_vuv = org_labels[:, 0, 0].astype(bool)
        org_labels = org_labels[:, 1:]
        len_diff = len(org_labels) - len(labels_post)
        org_labels = self.OutputGen.trim_end_sample(org_labels, int(len_diff / 2.0))
        org_labels = self.OutputGen.trim_end_sample(org_labels, int(len_diff / 2.0) + 1)
        org_atoms = AtomLabelGen.labels_to_atoms(org_labels, k=hparams.k, frame_size=hparams.frame_size_ms)
        wcad_lf0 = self.OutputGen.atoms_to_lf0(org_atoms, len(org_labels))

        # Get a data plotter
        net_name = os.path.basename(hparams.model_name)
        filename = str(os.path.join(hparams.out_dir, id_name + '.' + net_name))
        plotter = DataPlotter()
        plotter.set_title(id_name + " - " + net_name)

        grid_idx = 0
        graphs_output = list()
        for idx in reversed(range(output_amps.shape[1])):
            graphs_output.append((output_amps[:, idx], r'$\theta$={0:.3f}'.format(hparams.thetas[idx])))
        plotter.set_data_list(grid_idx=grid_idx, data_list=graphs_output)
        plotter.set_label(grid_idx=grid_idx, ylabel='NN amps')
        amp_max = np.max(output_amps) * 1.1
        amp_min = np.min(output_amps) * 1.1
        plotter.set_lim(grid_idx=grid_idx, ymin=amp_min, ymax=amp_max)

        grid_idx += 1
        graphs_pos_flag = list()
        graphs_pos_flag.append((output_pos,))
        plotter.set_data_list(grid_idx=grid_idx, data_list=graphs_pos_flag)
        plotter.set_label(grid_idx=grid_idx, ylabel='NN pos')

        grid_idx += 1
        graphs_peaks = list()
        for idx in reversed(range(label.shape[1] - 2)):
            graphs_peaks.append((labels_post[:, 1 + idx, 0],))
        plotter.set_data_list(grid_idx=grid_idx, data_list=graphs_peaks)
        plotter.set_area_list(grid_idx=grid_idx, area_list=[(np.invert(output_vuv), '0.75', 1.0, 'Unvoiced')])
        plotter.set_label(grid_idx=grid_idx, ylabel='NN peaks')
        plotter.set_lim(grid_idx=grid_idx, ymin=-1.8, ymax=1.8)

        grid_idx += 1
        graphs_target = list()
        for idx in reversed(range(org_labels.shape[1])):
            graphs_target.append((org_labels[:, idx, 0],))
        plotter.set_data_list(grid_idx=grid_idx, data_list=graphs_target)
        plotter.set_hatchstyles(grid_idx=grid_idx, hatchstyles=['\\\\'])
        plotter.set_area_list(grid_idx=grid_idx, area_list=[(np.invert(org_vuv.astype(bool)), '0.75', 1.0, 'Reference unvoiced')])
        plotter.set_label(grid_idx=grid_idx, ylabel='target')
        plotter.set_lim(grid_idx=grid_idx, ymin=-1.8, ymax=1.8)

        grid_idx += 1
        graphs_lf0 = list()
        graphs_lf0.append((wcad_lf0, "wcad lf0"))
        graphs_lf0.append((original_lf0, "org lf0"))
        graphs_lf0.append((output_lf0, "predicted lf0"))
        plotter.set_data_list(grid_idx=grid_idx, data_list=graphs_lf0)
        plotter.set_area_list(grid_idx=grid_idx, area_list=[(np.invert(org_vuv.astype(bool)), '0.75', 1.0)])
        plotter.set_hatchstyles(grid_idx=grid_idx, hatchstyles=['\\\\'])
        plotter.set_label(grid_idx=grid_idx, xlabel='frames [' + str(hparams.frame_size_ms) + ' ms]', ylabel='lf0')
        amp_lim = max(np.max(np.abs(wcad_lf0)), np.max(np.abs(output_lf0))) * 1.1
        plotter.set_lim(grid_idx=grid_idx, ymin=-amp_lim, ymax=amp_lim)
        plotter.set_linestyles(grid_idx=grid_idx, linestyles=[':', '--', '-'])

        # # Compute F0 RMSE for sample and add it to title.
        # org_f0 = (np.exp(lf0.squeeze() + phrase_curve[:len(lf0)].squeeze()) * vuv)[:len(output_lf0)]  # Fix minor negligible length mismatch.
        # output_f0 = np.exp(output_lf0 + phrase_curve[:len(output_lf0)].squeeze()) * output_vuv[:len(output_lf0)]
        # f0_mse = (org_f0 - output_f0) ** 2
        # # non_zero_count = np.logical_and(vuv[:len(output_lf0)], output_vuv).sum()
        # f0_rmse = math.sqrt(f0_mse.sum() / (np.logical_and(vuv[:len(output_lf0)], output_vuv).sum()))

        # # Compute vuv error rate.
        # num_errors = (vuv[:len(output_lf0)] != output_vuv)
        # vuv_error_rate = float(num_errors.sum()) / len(output_lf0)
        # plotter.set_title(id_name + " - " + net_name + " - F0_RMSE_" + "{:4.2f}Hz".format(f0_rmse) + " - VUV_" + "{:2.2f}%".format(vuv_error_rate * 100))
        # plotter.set_lim(xmin=300, xmax=1100)g
        plotter.gen_plot(monochrome=True)
        plotter.gen_plot()
        plotter.save_to_file(filename + ".VUV_DIST_POS.png")

    def compute_score(self, dict_outputs_post, dict_hiddens, hparams):
        """Compute the score of a dictionary with post-processes labels."""

        # Get data for comparision.
        dict_original_post = self.load_extracted_audio_features(dict_outputs_post, hparams)

        f0_rmse = 0.0
        vuv_error_rate = 0.0
        f0_rmse_max_id = "None"
        f0_rmse_max = 0.0
        vuv_error_max_id = "None"
        vuv_error_max = 0.0
        for id_name, label in dict_outputs_post.items():

            output_vuv = label[:, 0, 1].astype(bool)
            output_atom_labels = label[:, 1:]
            output_lf0 = self.OutputGen.labels_to_lf0(output_atom_labels, k=hparams.k, frame_size=hparams.frame_size_ms, amp_threshold=hparams.min_atom_amp)

            # Get data for comparision.
            org_lf0 = dict_original_post[id_name][:, 60]
            org_vuv = dict_original_post[id_name][:, 61]
            phrase_curve = self.get_phrase_curve(id_name)

            # Compute f0 from lf0.
            org_f0 = np.exp(org_lf0.squeeze())[:len(output_lf0)]  # Fix minor negligible length mismatch.
            output_f0 = np.exp(output_lf0 + phrase_curve[:len(output_lf0)].squeeze())

            # Compute RMSE, keep track of worst RMSE.
            f0_mse = (org_f0 - output_f0) ** 2
            current_f0_rmse = math.sqrt((f0_mse * org_vuv[:len(output_lf0)]).sum() / org_vuv[:len(output_lf0)].sum())
            if current_f0_rmse > f0_rmse_max:
                f0_rmse_max_id = id_name
                f0_rmse_max = current_f0_rmse
            f0_rmse += current_f0_rmse

            # Compute vuv error rate.
            num_errors = (org_vuv[:len(output_lf0)] != output_vuv)
            vuv_error_rate_tmp = float(num_errors.sum()) / len(output_lf0)
            if vuv_error_rate_tmp > vuv_error_max:
                vuv_error_max_id = id_name
                vuv_error_max = vuv_error_rate_tmp
            vuv_error_rate += vuv_error_rate_tmp

        f0_rmse /= len(dict_outputs_post)
        vuv_error_rate /= len(dict_outputs_post)

        self.logger.info("Worst F0 RMSE: " + f0_rmse_max_id + " {:4.2f}Hz".format(f0_rmse_max))
        self.logger.info("Worst VUV error: " + vuv_error_max_id + " {:2.2f}%".format(vuv_error_max * 100))
        self.logger.info("Benchmark score: F0 RMSE " + "{:4.2f}Hz".format(f0_rmse)
                         + ", VUV " + "{:2.2f}%".format(vuv_error_rate * 100))

        return f0_rmse, vuv_error_rate

    def synthesize(self, id_list, synth_output, hparams):
        """This method should be overwritten by sub classes."""
        full_output = self.run_atom_synth(id_list, synth_output, hparams)

        for id_name, labels in full_output.items():
            lf0 = labels[:, -3]
            lf0 = interpolate_lin(lf0)
            vuv = synth_output[id_name][:, 0, 1]
            len_diff = len(labels) - len(vuv)
            labels = WorldFeatLabelGen.trim_end_sample(labels, int(len_diff / 2), reverse=True)
            labels = WorldFeatLabelGen.trim_end_sample(labels, len_diff - int(len_diff / 2))
            labels[:, -2] = vuv

        # Run the vocoder.
        ModelTrainer.synthesize(self, id_list, full_output, hparams)

# def main():
#     logging.basicConfig(level=logging.INFO)
#
#     parser = argparse.ArgumentParser(description=__doc__, formatter_class=argparse.ArgumentDefaultsHelpFormatter)
#     parser.add_argument("-w", "--wcad_root", help="Directory with the WCAD scripts.",
#                         type=str, dest="wcad_root", default=argparse.SUPPRESS, required=True)
#     parser.add_argument("-m", "--merlin_src_root", help="Path of src directory of merlin.",
#                         type=str, dest="merlin_src_root", default=argparse.SUPPRESS, required=True)
#     parser.add_argument("-a", "--audio_dir", help="Directory containing the audio (wav) files.",
#                         type=str, dest="audio_dir", default=argparse.SUPPRESS, required=True)
#     parser.add_argument("-l", "--label_dir", help="Directory containing the label (HTK full labels, *.lab) files.",
#                         type=str, dest="label_dir", default=argparse.SUPPRESS, required=True)
#     parser.add_argument("-o", "--out_dir", help="Working directory for loading and saving.",
#                         type=str, dest="out_dir", default=argparse.SUPPRESS, required=True)
#
#     parser.add_argument("-q", "--question_file", help="Full path to question file.",
#                         type=str, dest="question_file", default=argparse.SUPPRESS, required=True)
#     parser.add_argument("--acoustic_model_name",
#                         help="Name of the acoustic model in out_dir used to generate bap and mcep.",
#                         type=str, dest="acoustic_model_name", default=None, required=True)
#     parser.add_argument("-i", "--file_id_list_path",
#                         help="Path to a text file to read the ids of the files to process.",
#                         type=str, dest="file_id_list_path", default=argparse.SUPPRESS, required=True)
#     parser.add_argument("-b", "--file_id_list_benchmark_path",
#                         help="Path to a text file to read the ids of the files to benchmark on.",
#                         type=str, dest="file_id_list_benchmark_path", default=argparse.SUPPRESS, required=True)
#
#     parser.add_argument("--theta_start", help="Start value of theta.",
#                         type=float, dest="theta_start", default=0.01)
#     parser.add_argument("--theta_stop", help="Stop value of theta (excluded).",
#                         type=float, dest="theta_stop", default=0.055)
#     parser.add_argument("--theta_step", help="Distance between the thetas.",
#                         type=float, dest="theta_step", default=0.005)
#
#     parser.add_argument("-s", "--sampling_frequency", help="Sampling frequency of all audio files [Hz].",
#                         type=int, dest="sampling_frequency", choices=[16000, 48000])
#     parser.add_argument("-f", "--frame_size_ms", help="Frame size of the labels.",
#                         type=int, dest="frame_size_ms", default=5)
#     parser.add_argument("--dist_window_size", help="Width of the distribution surrounding each atom spike.",
#                         type=int, dest="dist_window_size", default=51)
#
#     parser.add_argument("--compute_labels_input",
#                         help="If set, the labels will be recomputed "
#                              "and not loaded from the files in the directory given in -o/--out_dir.",
#                         dest="compute_labels_input", action='store_const', const=True, default=False)
#     parser.add_argument("--compute_labels_output",
#                         help="If set, the labels will be recomputed "
#                              "and not loaded from the files in the directory given in -o/--out_dir.",
#                         dest="compute_labels_output", action='store_const', const=True, default=False)
#
#     parser.add_argument("--use_gpu", help="If set computation is done on a CUDA device.",
#                         dest="use_gpu", action='store_const', const=True, default=False)
#     parser.add_argument("--epochs",
#                         help="Number of training epochs. If 0 no training is performed and training data isn't loaded.",
#                         type=int, dest="epochs", default=argparse.SUPPRESS, required=True)
#     parser.add_argument("--model_name",
#                         help="The name used to save the model. "
#                              "The model is only saved if training was performed (epochs > 0). "
#                              "If --model_type is not set this name is used to load the model.",
#                         type=str, dest="model_name", default=argparse.SUPPRESS, required=True)
#     parser.add_argument("--model_type",
#                         help="Name of the architecture used for the model. "
#                              "If this is not set the model is loaded by its --model_name.",
#                         type=str, dest="model_type", default=argparse.SUPPRESS, required=False)
#     parser.add_argument("--batch_size",
#                         help="Batch_size is applied to frames not utterances. "
#                              "When RNNs are used all frames will be processed jointly, so batch_size is ignored.",
#                         type=int, dest="batch_size", default=256)
#     parser.add_argument("--learning_rate",
#                         help="Learning will start with this learning rate but will decrease on plateaus.",
#                         type=float, dest="learning_rate", default=0.002)
#     parser.add_argument("--momentum", help="Note that some optimizers will not use a momentum value.",
#                         type=float, dest="momentum", default=0.8)
#     parser.add_argument("--optim", help="Name of the optimiser_type to use.",
#                         choices=["Adam", "SGD"],
#                         type=str, dest="optim", default="Adam")
#
#     # Parse arguments.
#     args = parser.parse_args()
#
#     # Environment/data arguments.
#     wcad_root = os.path.abspath(args.wcad_root)
#     merlin_src_root = os.path.abspath(args.merlin_src_root)
#     audio_dir = os.path.abspath(args.audio_dir)
#     label_dir = os.path.abspath(args.label_dir)
#     out_dir = os.path.abspath(args.out_dir)
#
#     question_file = os.path.abspath(args.question_file)
#     acoustic_model_name = args.acoustic_model_name if args.acoustic_model_name != "None" else None
#     file_id_list_path = os.path.abspath(args.file_id_list_path)
#     file_id_list_benchmark_path = os.path.abspath(args.file_id_list_benchmark_path)
#
#     theta_start = args.theta_start
#     theta_step = args.theta_step
#     theta_stop = args.theta_stop
#
#     sampling_frequency = args.sampling_frequency
#     frame_size_ms = args.frame_size_ms
#     if frame_size_ms != parser.get_default("frame_size_ms"):
#         logging.error("Merlin supports only a frame size of 5 ms.")
#         sys.exit(1)
#     dist_window_size = args.dist_window_size
#
#     compute_labels_input = args.compute_labels_input
#     compute_labels_output = args.compute_labels_output
#     model_type = args.model_type if (hasattr(args, 'model_type') and args.model_type != "None") else None
#     model_name = args.model_name
#
#     # Training arguments.
#     use_gpu = args.use_gpu
#     batch_size = args.batch_size
#     learning_rate = args.learning_rate
#     momentum = args.momentum
#     optim = args.optim
#     epochs = max(0, args.epochs)
#
#     # Read which files to process.
#     with open(file_id_list_path) as f:
#         file_id_list = f.readlines()
#     # Trim entries in-place.
#     file_id_list[:] = [s.strip(' \t\n\r') for s in file_id_list]
#
#     atom_model_trainer = WcadAtomVUVDistPosModelTrainer(merlin_src_root, wcad_root, audio_dir, label_dir, out_dir,
#                                                         theta_start, theta_stop, theta_step,
#                                                         question_file, acoustic_model_name, file_id_list,
#                                                         sampling_frequency, frame_size_ms,
#                                                         (not compute_labels_input), (not compute_labels_output),
#                                                         dist_window_size)
#     atom_model_trainer.train(epochs, use_gpu, model_type, model_name, batch_size, learning_rate, momentum, optim)
#
#     # Read which files to benchmark on.
#     with open(file_id_list_benchmark_path) as f:
#         file_id_list_benchmark = f.readlines()
#     # Trim entries in-place.
#     file_id_list_benchmark[:] = [s.strip(' \t\n\r') for s in file_id_list_benchmark]
#     # atom_model_trainer.benchmark(file_id_list_benchmark)
#
#     synth_file_id_list = ["roger_5335", "roger_5540", "roger_5541", "roger_5542", "roger_5543", "roger_5544"]
#     # synth_file_id_list = ["roger_10661", "roger_10727", "roger_10725"]
#     atom_model_trainer.synth(synth_file_id_list, model_name=model_name)
#
#
# if __name__ == "__main__":
#     main()
